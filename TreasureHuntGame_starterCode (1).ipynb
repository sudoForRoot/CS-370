{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunt Game Notebook\n",
    "\n",
    "## Read and Review Your Starter Code\n",
    "The theme of this project is a popular treasure hunt game in which the player needs to find the treasure before the pirate does. While you will not be developing the entire game, you will write the part of the game that represents the intelligent agent, which is a pirate in this case. The pirate will try to find the optimal path to the treasure using deep Q-learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black;\">\n",
    "<b>To Begin:</b> Use this <b>TreasureHuntGame_starterCode.ipynb</b> file to complete your assignment. \n",
    "<br><br>\n",
    "You have been provided with two Python classes and this notebook to help you with this assignment. The first class, <b>TreasureMaze.py</b>, represents the environment, which includes a maze object defined as a matrix. The second class, <b>GameExperience.py</b>, stores the episodes ‚Äì that is, all the states that come in between the initial state and the terminal state. This is later used by the agent for learning by experience, called \"exploration\". This notebook shows how to play a game. Your task is to complete the deep Q-learning implementation in the qtrain() function for which a skeleton implementation has been provided. \n",
    "</div>\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black;\">\n",
    "<b>NOTE: </b>The code block you will need to complete will have <b>#TODO</b> as a header.\n",
    "<br> First, read and review the next few code and instruction blocks to understand the code that you have been given.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color: #333333;\">\n",
    "<b>Installations</b> The following command will install the necessary Python libraries to necessary to run this application. If you see a \"[notice] A new release of pip is available: 23.1.2 -> 25.2\" at the end of the installation, you may disregard that statement. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tensorflow CPU Acceleration Warning</h2>\n",
    "<div class=\"alert alert-block alert-danger\" style=\"color: #333333;\">\n",
    "<b>GPU/CUDA/Memory Warnings/Errors:</b> You may receive some errors referencing that GPUs will not be used, CUDA could not be found, or free system memory allocation errors. These and a few others, are standard errors that can be ignored here as they are environment based.<br><br>\n",
    "    <b>Example messages:</b>\n",
    "    <ul>\n",
    "        <li>oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders</li>\n",
    "        <li>WARNING: All log messages before absl::InitializeLog() is called are written to STDERR</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 0: TensorFlow Configuration\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress most warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN if causing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Imports\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from TreasureMaze import TreasureMaze\n",
    "from GameExperience import GameExperience\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU config error: {e}\")\n",
    "\n",
    "# Clear session\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Maze Object Generation</h2>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black;\">\n",
    "    <b>NOTE:</b>  The following code block contains an 8x8 matrix that will be used as a maze object:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Maze Definition\n",
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "print(\"Maze shape:\", maze.shape)\n",
    "print(\"Number of free cells:\", np.sum(maze == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions and Global Variables</h2>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black;\">\n",
    "This <b>show()</b> helper function allows a visual representation of the maze object:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Visualization Function\n",
    "def show(qmaze):\n",
    "    \"\"\"Display the current maze state with pirate position and visited cells\"\"\"\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    pirate_row, pirate_col, _ = qmaze.state\n",
    "    canvas[pirate_row, pirate_col] = 0.3   # pirate cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # treasure cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>pirate agent</b> can move in four directions: left, right, up, and down. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"color:black;\">\n",
    "<b>Note:</b> While the agent primarily learns by experience through exploitation, often, the agent can choose to explore the environment to find previously undiscovered paths. This is called \"exploration\" and is defined by epsilon. This value is the <b>EXPLORATION</b> values from the Cartpole assignment. The hyperparameters are provided here and used in the <b>qtrain()</b> method. \n",
    "You are encouraged to try various values for the exploration factor and see how the algorithm performs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Action Constants\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "print(f\"Number of possible actions: {num_actions}\")\n",
    "print(f\"Actions: {actions_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample code block and output below show creating a maze object and performing one action (DOWN), which returns the reward. The resulting updated environment is visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Test Environment\n",
    "qmaze = TreasureMaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"Test action - Moving DOWN\")\n",
    "print(\"reward=\", reward)\n",
    "print(\"game_over=\", game_over)\n",
    "show(qmaze)\n",
    "plt.title(\"Initial Maze Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black;\">\n",
    "    <b>NOTE:</b> This <b>play_game()</b> function simulates a full game based on the provided trained model. The other parameters include the TreasureMaze object, the starting position of the pirate and max amount of steps to make sure the code does not get stuck in a loop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Play Game Function\n",
    "def play_game(model, qmaze, pirate_cell, max_steps=None):\n",
    "    \"\"\"Play a single game from starting cell and return win status\"\"\"\n",
    "    qmaze.reset(pirate_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    steps = 0\n",
    "    if max_steps is None:\n",
    "        max_steps = qmaze.maze.size * 4  # safety cutoff\n",
    "\n",
    "    while steps < max_steps:\n",
    "        state = np.asarray(envstate, dtype=np.float32)\n",
    "        if state.ndim == 1:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "        q_values = model(state, training=False).numpy()\n",
    "        action = np.argmax(q_values[0])\n",
    "\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        steps += 1\n",
    "\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "\n",
    "    return False  # timed out with no result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black;\">\n",
    "<b>Note: </b>\n",
    "    This <b>completion_check()</b> function helps you to determine whether the pirate can win any game at all. If your maze is not well designed, the pirate may not win any game at all. In this case, your training would not yield any result. The provided maze in this notebook ensures that there is a path to win and you can run this method to check.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Completion Check Function\n",
    "def completion_check(model, maze_or_qmaze, max_steps=None):\n",
    "    \"\"\"Check if agent can win from all free cells\"\"\"\n",
    "    # Accept either raw numpy maze or TreasureMaze instance\n",
    "    if isinstance(maze_or_qmaze, TreasureMaze):\n",
    "        qmaze = maze_or_qmaze\n",
    "    else:\n",
    "        qmaze = TreasureMaze(maze_or_qmaze)\n",
    "\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            continue\n",
    "        if not play_game(model, qmaze, cell, max_steps=max_steps):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black;\">\n",
    "<b>Note: </b>\n",
    "</b>The <b>build_model()</b> function in the block below will build the neural network model. Review the code and note the number of layers, as well as the activation, optimizer, and loss functions that are used to train the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Build Model Function\n",
    "def build_model(maze):\n",
    "    \"\"\"Build the neural network model for Q-learning\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: Helper function for time formatting\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format time in seconds to readable string\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds // 60\n",
    "        secs = seconds % 60\n",
    "        return f\"{int(minutes)}m {int(secs)}s\"\n",
    "    else:\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        return f\"{int(hours)}h {int(minutes)}m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black;\">\n",
    "    <b>Note:</b>\n",
    "    This <b>train_step()</b> helper function in the block below is used to help predict Q-values (quality values) in the current modelto see how good each action is in a given state and improve the Q-network by reducing the gap between what is predicted and what should have been predicted. \n",
    "</div>\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black;\">\n",
    "If you're interested in reading up on the <i>@tf.function</i>, which is a decorator for Tensorflow to run this code into a TensorFlow computation graph, please refer to this link: <a href=\"https://www.tensorflow.org/guide/intro_to_graphs\">https://www.tensorflow.org/guide/intro_to_graphs</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tensorflow GPU Warning</h2>\n",
    "<div class=\"alert alert-block alert-danger\" style=\"color: #333333;\">\n",
    "    You will see a <b>warning in red</b> \"INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.\". This is simply coming from <b>Tensorflow skipping using GPU for this assignment.</b>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError()\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mfunction\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(x, y):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Block 0: TensorFlow Configuration (PUT THIS FIRST)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress most warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN if causing issues\n",
    "\n",
    "# Block 1: Imports\n",
    "from __future__ import print_function\n",
    "import datetime, json, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, PReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from TreasureMaze import TreasureMaze\n",
    "from GameExperience import GameExperience\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU config error: {e}\")\n",
    "\n",
    "# Clear session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Block 2-8: Same as before (maze definition, functions, etc.)\n",
    "# ... [Your existing Blocks 2-8] ...\n",
    "\n",
    "# Block 9: Fixed Model and Training Components\n",
    "print(\"\\nInitializing model and training components...\")\n",
    "\n",
    "# Build the model\n",
    "model = build_model(maze)\n",
    "print(\"Model summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Safe training function without @tf.function decorator initially\n",
    "def train_step_safe(x, y):\n",
    "    \"\"\"Training step without @tf.function to avoid graph compilation issues\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(x, training=True)\n",
    "        loss = loss_fn(y, q_values)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # Clip gradients to prevent explosions\n",
    "    grads = [tf.clip_by_norm(g, 1.0) for g in grads]\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "print(\"Training components initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: Complete the Q-Training Algorithm Code Block\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black;\">\n",
    "    This is your deep Q-learning implementation. The goal of your deep Q-learning implementation is to find the best possible navigation sequence that results in reaching the treasure cell while maximizing the reward. In your implementation, you need to determine the optimal number of epochs to achieve a 100% win rate.\n",
    "</div>\n",
    "    <b>Pseudocode:</b>\n",
    "    <br>\n",
    "    For each epoch:\n",
    "        Reset the environment at a random starting cell\n",
    "        agent_cell = randomly select a free cell\n",
    "        <br>\n",
    "        <b>Hint:</b> Review the reset method in the TreasureMaze.py class.\n",
    "    \n",
    "        Set the initial environment state\n",
    "        env_state should reference the environment's current state\n",
    "        Hint: Review the observe method in the TreasureMaze.py class.\n",
    "\n",
    "        While game status is not game over:\n",
    "           previous_envstate = env_state\n",
    "            Decide on an action:\n",
    "                - If possible, take a random valid exploration action and \n",
    "                  randomly choose action (left, right, up, down)\n",
    "                  and assign it to an action variable\n",
    "                - Else, pick the best exploitation action from the model and assign it to an action variable\n",
    "                  Hint: Review the predict method in the GameExperience.py class.\n",
    "    \n",
    "           Retrieve the values below from the act() method.\n",
    "           env_state, reward, game_status = qmaze.act(action)\n",
    "           Hint: Review the act method in the TreasureMaze.py class.\n",
    "    \n",
    "            Track the wins and losses from the game_status using win_history \n",
    "         \n",
    "           Store the episode below in the Experience replay object\n",
    "           episode = [previous_envstate, action, reward, envstate, game_status]\n",
    "           Hint: Review the remember method in the GameExperience.py class.\n",
    "        \n",
    "           Train neural network model and evaluate loss\n",
    "           Hint: Call GameExperience.get_data to retrieve training data (input and target) \n",
    "           and pass to the train_step method and assign it to batch_loss and append to the loss variable\n",
    "        \n",
    "      If the win rate is above the threshold and your model passes the completion check, that would be your epoch.\n",
    "\n",
    "Note: A 100% win rate <b>DOES NOT EXPLICITLY MEAN</b> that you have solved the maze. It simply indicates that during the last evaluation, the pirate <i>happened</i> to get to the treasure. Be sure to utilise the <b>completion_check()</b> function to validate your pirate found the treasure at every starting point and consistently! \n",
    "\n",
    "<b> You will need to complete the section starting with #START_HERE. Please use the pseudocode above as guidance. </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE Q-TRAINING ALGORITHM IMPLEMENTATION\n",
    "# This cell completes the required TODO section\n",
    "# ============================================\n",
    "\n",
    "def qtrain(model, maze, **opt):\n",
    "    \"\"\"\n",
    "    Complete Q-training algorithm for the pirate intelligent agent.\n",
    "    \n",
    "    This function implements deep Q-learning with experience replay and \n",
    "    target networks to train an agent to navigate the maze and find treasure.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The neural network model for Q-value approximation\n",
    "    - maze: The maze environment\n",
    "    - **opt: Optional hyperparameters:\n",
    "        - n_epoch: Maximum number of training epochs\n",
    "        - max_memory: Size of experience replay buffer\n",
    "        - data_size: Batch size for training\n",
    "        - target_update_freq: Frequency of target network updates\n",
    "        - gamma: Discount factor for future rewards\n",
    "    \n",
    "    Returns:\n",
    "    - win_history: List of wins (1) and losses (0) per epoch\n",
    "    - model: Trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================\n",
    "    # HYPERPARAMETERS\n",
    "    # ============================================\n",
    "    \n",
    "    # Get hyperparameters from options or use defaults\n",
    "    n_epoch = opt.get('n_epoch', 1500)\n",
    "    max_memory = opt.get('max_memory', 8 * maze.size)\n",
    "    data_size = opt.get('data_size', 32)\n",
    "    target_update_freq = opt.get('target_update_freq', 10)\n",
    "    \n",
    "    # Exploration parameters\n",
    "    epsilon = 1.0  # Start with 100% exploration\n",
    "    epsilon_decay = 0.998  # Decay rate per epoch\n",
    "    epsilon_min = 0.01  # Minimum exploration rate\n",
    "    \n",
    "    # Reinforcement learning parameters\n",
    "    gamma = opt.get('gamma', 0.95)  # Discount factor\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    target_win_rate = 0.95  # Target win rate (95%)\n",
    "    patience = 50  # Epochs to wait for improvement\n",
    "    \n",
    "    # ============================================\n",
    "    # INITIALIZATION\n",
    "    # ============================================\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    qmaze = TreasureMaze(maze)\n",
    "    print(f\"Initializing training at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Maze size: {maze.shape}, Free cells: {len(qmaze.free_cells)}\")\n",
    "    print(f\"Training parameters: epochs={n_epoch}, memory={max_memory}, batch={data_size}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create target network for stable training\n",
    "    target_model = clone_model(model)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    # Initialize experience replay\n",
    "    experience = GameExperience(model, max_memory=max_memory)\n",
    "    \n",
    "    # Training statistics\n",
    "    win_history = []  # Track wins (1) and losses (0)\n",
    "    loss_history = []  # Track loss values\n",
    "    episode_steps = []  # Track steps per episode\n",
    "    best_win_rate = 0.0\n",
    "    no_improvement_count = 0\n",
    "    optimal_epoch = None\n",
    "    \n",
    "    # ============================================\n",
    "    # MAIN TRAINING LOOP\n",
    "    # ============================================\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        # --- Step 1: Reset environment at random starting cell ---\n",
    "        agent_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(agent_cell)\n",
    "        \n",
    "        # --- Step 2: Get initial environment state ---\n",
    "        env_state = qmaze.observe()\n",
    "        \n",
    "        # Episode tracking\n",
    "        game_status = None\n",
    "        episode_loss = []\n",
    "        steps = 0\n",
    "        max_steps_per_episode = 200  # Prevent infinite loops\n",
    "        \n",
    "        # --- Step 3: Play episode until game over ---\n",
    "        while game_status != 'win' and game_status != 'lose' and steps < max_steps_per_episode:\n",
    "            steps += 1\n",
    "            prev_envstate = env_state\n",
    "            \n",
    "            # --- Step 4: Action selection (epsilon-greedy) ---\n",
    "            \n",
    "            # Get valid actions from current state\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            \n",
    "            # Exploration: take random valid action\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            # Exploitation: use model to predict best action\n",
    "            else:\n",
    "                # Prepare state for model\n",
    "                state_tensor = np.asarray(prev_envstate).reshape(1, -1)\n",
    "                # Predict Q-values using target model for stability\n",
    "                q_values = target_model.predict(state_tensor, verbose=0)[0]\n",
    "                \n",
    "                # Select best action from valid actions only\n",
    "                valid_q_values = [(a, q_values[a]) for a in valid_actions]\n",
    "                action = max(valid_q_values, key=lambda x: x[1])[0]\n",
    "            \n",
    "            # --- Step 5: Take action and observe result ---\n",
    "            env_state, reward, game_status = qmaze.act(action)\n",
    "            \n",
    "            # --- Step 6: Store episode in experience replay ---\n",
    "            episode = [prev_envstate, action, reward, env_state, game_status]\n",
    "            experience.remember(episode)\n",
    "            \n",
    "            # --- Step 7: Train neural network with experience replay ---\n",
    "            if len(experience.memory) > data_size:\n",
    "                # Get training data from experience replay\n",
    "                inputs, targets = experience.get_data(data_size=data_size)\n",
    "                \n",
    "                # Convert to tensors if needed\n",
    "                if not isinstance(inputs, np.ndarray):\n",
    "                    inputs = np.array(inputs)\n",
    "                if not isinstance(targets, np.ndarray):\n",
    "                    targets = np.array(targets)\n",
    "                \n",
    "                # Train the model\n",
    "                batch_loss = model.train_on_batch(inputs, targets)\n",
    "                episode_loss.append(batch_loss)\n",
    "        \n",
    "        # --- Step 8: Track wins and losses ---\n",
    "        if game_status == 'win':\n",
    "            win_history.append(1)\n",
    "        else:\n",
    "            win_history.append(0)\n",
    "        \n",
    "        episode_steps.append(steps)\n",
    "        \n",
    "        # Record average loss for this episode\n",
    "        if episode_loss:\n",
    "            loss_history.append(np.mean(episode_loss))\n",
    "        else:\n",
    "            loss_history.append(0)\n",
    "        \n",
    "        # --- Step 9: Update target network periodically ---\n",
    "        if epoch % target_update_freq == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "        \n",
    "        # --- Step 10: Decay exploration rate ---\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        # ============================================\n",
    "        # PROGRESS MONITORING AND EARLY STOPPING\n",
    "        # ============================================\n",
    "        \n",
    "        # Calculate win rate over recent episodes\n",
    "        window_size = min(100, len(win_history))\n",
    "        if window_size > 0:\n",
    "            current_win_rate = np.mean(win_history[-window_size:])\n",
    "        else:\n",
    "            current_win_rate = 0.0\n",
    "        \n",
    "        # Progress reporting every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            avg_steps = np.mean(episode_steps[-100:]) if len(episode_steps) >= 100 else np.mean(episode_steps)\n",
    "            avg_loss = np.mean(loss_history[-100:]) if loss_history else 0\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:4d}/{n_epoch} | \"\n",
    "                  f\"Win Rate: {current_win_rate:.3f} | \"\n",
    "                  f\"Steps: {avg_steps:.1f} | \"\n",
    "                  f\"Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Epsilon: {epsilon:.3f} | \"\n",
    "                  f\"Time: {format_time(elapsed_time.total_seconds())}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if current_win_rate > best_win_rate:\n",
    "            best_win_rate = current_win_rate\n",
    "            no_improvement_count = 0\n",
    "            \n",
    "            # Save best model when win rate exceeds threshold\n",
    "            if current_win_rate >= target_win_rate:\n",
    "                model.save(\"best_pirate_model.h5\")\n",
    "                print(f\"\\n‚úì Saved best model with win rate {current_win_rate:.3f} at epoch {epoch+1}\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "        \n",
    "        # Check if agent can solve maze from all starting positions\n",
    "        if current_win_rate >= 0.99 and epoch >= 500:\n",
    "            print(f\"\\nRunning completion check at epoch {epoch+1}...\")\n",
    "            if completion_check(model, qmaze, max_steps=200):\n",
    "                print(f\"‚úì AGENT SUCCESSFULLY SOLVES MAZE FROM ALL POSITIONS at epoch {epoch+1}!\")\n",
    "                optimal_epoch = epoch + 1\n",
    "                break\n",
    "        \n",
    "        # Early stopping if no improvement for many epochs\n",
    "        if no_improvement_count >= patience and epoch >= 500:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    # ============================================\n",
    "    # FINAL EVALUATION\n",
    "    # ============================================\n",
    "    \n",
    "    total_time = datetime.datetime.now() - start_time\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total epochs: {epoch+1}\")\n",
    "    print(f\"Total training time: {format_time(total_time.total_seconds())}\")\n",
    "    print(f\"Final win rate (last 100): {np.mean(win_history[-100:]):.3f}\")\n",
    "    print(f\"Best win rate achieved: {best_win_rate:.3f}\")\n",
    "    \n",
    "    # Final completion check\n",
    "    print(\"\\nRunning final completion check...\")\n",
    "    if completion_check(model, qmaze, max_steps=200):\n",
    "        print(\"‚úì SUCCESS: Agent can find treasure from all starting positions!\")\n",
    "    else:\n",
    "        print(\"‚úó NOTE: Agent cannot yet solve maze from all starting positions\")\n",
    "        print(\"  Consider training longer or adjusting hyperparameters\")\n",
    "    \n",
    "    # Return training history and model\n",
    "    return win_history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Model\n",
    "\n",
    "Now we will start testing the deep Q-learning implementation. To begin, select **Cell**, then **Run All** from the menu bar. This will run your notebook. As it runs, you should see output begin to appear beneath the next few cells. The code below creates an <b>instance</b> of TreasureMaze. This does not show your actual training done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST YOUR MODEL\n",
    "# ============================================\n",
    "# Now we will start testing the deep Q-learning implementation.\n",
    "# To begin, select Cell, then Run All from the menu bar. \n",
    "# This will run your notebook. As it runs, you should see output \n",
    "# begin to appear beneath the next few cells.\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING PHASE: Evaluating the Trained Pirate Agent\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This section tests the deep Q-learning implementation by\")\n",
    "print(\"evaluating the trained agent's performance in the maze.\")\n",
    "print(\"\\nThe code below creates instances of TreasureMaze and runs\")\n",
    "print(\"comprehensive tests to verify the agent's learning.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you will build your model using the <b>build_model</b> function and train it using deep Q-learning. Note: This step takes several minutes to fully run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color: #333333;\">\n",
    "  <b>WARNING</b>  If you did not attempt the assignment, the code <b>will</b> error out at this section.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BUILD AND TRAIN MODEL WITH DEEP Q-LEARNING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING NEURAL NETWORK MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build the model using the build_model function\n",
    "print(\"\\n[Step 1] Creating neural network architecture...\")\n",
    "model = build_model(maze)\n",
    "\n",
    "print(\"\\n‚úì Model built successfully!\")\n",
    "print(\"Model Architecture:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Input layer: {maze.size} neurons (flattened maze state)\")\n",
    "print(f\"  Hidden layer 1: {maze.size} neurons with PReLU activation\")\n",
    "print(f\"  Hidden layer 2: {maze.size} neurons with PReLU activation\")  \n",
    "print(f\"  Output layer: {num_actions} neurons (Q-values for each action)\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss function: Mean Squared Error (MSE)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\nDetailed Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING WITH DEEP Q-LEARNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: This training step will take several minutes to complete.\")\n",
    "print(\"   The duration depends on:\")\n",
    "print(\"   - Number of epochs (1000)\")\n",
    "print(\"   - Maze complexity (8x8 grid with obstacles)\")\n",
    "print(\"   - Your computer's processing power\")\n",
    "print(\"\\n   Expected training time: 5-15 minutes\")\n",
    "print(\"   Please be patient and do not interrupt the kernel.\\n\")\n",
    "\n",
    "print(\"Training Parameters:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  Epochs: 1000\")\n",
    "print(\"  Exploration rate (Œµ): Starts at 1.0, decays to 0.01\")\n",
    "print(\"  Experience replay memory: 512 experiences\")\n",
    "print(\"  Batch size: 32\")\n",
    "print(\"  Target network update frequency: Every 10 epochs\")\n",
    "print(\"  Discount factor (Œ≥): 0.95\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING...\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nYou will see progress updates every 100 epochs below:\")\n",
    "print(\"- Win rate: Percentage of successful treasure finds\")\n",
    "print(\"- Steps: Average steps taken to reach treasure\")\n",
    "print(\"- Loss: Neural network error (should decrease over time)\")\n",
    "print(\"- Epsilon: Exploration rate (decays as agent learns)\")\n",
    "print(\"- Time: Elapsed training time\\n\")\n",
    "\n",
    "# Record start time\n",
    "training_start = time.time()\n",
    "\n",
    "# Train the model using deep Q-learning\n",
    "try:\n",
    "    win_history, trained_model = qtrain(model, maze, n_epoch=1000)\n",
    "    \n",
    "    # Calculate total training time\n",
    "    training_time = time.time() - training_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal training time: {format_time(training_time)}\")\n",
    "    print(f\"Total episodes completed: {len(win_history)}\")\n",
    "    print(f\"Final win rate: {np.mean(win_history[-100:]):.3f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    model.save(\"trained_pirate_model.h5\")\n",
    "    print(\"\\n‚úì Model saved as 'trained_pirate_model.h5'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Error during training: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check that all required functions are defined\")\n",
    "    print(\"2. Verify the maze environment is working\")\n",
    "    print(\"3. Try reducing n_epoch to 100 for faster testing\")\n",
    "    print(\"4. Ensure you have sufficient memory available\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT STEP: Test your trained model in the following cells\")\n",
    "print(\"=\" * 70)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black;\">\n",
    "<b>Note: </b> This cell will check to see if the model passes the completion check. Note: This could take several minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETION CHECK - TEST ALL STARTING POSITIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETION CHECK: Testing All Starting Positions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: This completion check will take several minutes to run.\")\n",
    "print(\"   The agent will be tested from EVERY possible starting position\")\n",
    "print(\"   in the maze to verify it can find the treasure consistently.\")\n",
    "print(\"\\n   What this test does:\")\n",
    "print(\"   - Iterates through all free cells in the maze\")\n",
    "print(\"   - For each starting position, plays a complete game\")\n",
    "print(\"   - Verifies the agent can reach the treasure\")\n",
    "print(\"   - Reports success only if ALL positions are solved\")\n",
    "print(\"\\n   Expected duration: 2-5 minutes (depending on maze size)\")\n",
    "print(\"   Please be patient and do not interrupt the kernel.\\n\")\n",
    "\n",
    "print(\"Maze Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "# Create a temporary maze to count free cells\n",
    "temp_maze = TreasureMaze(maze)\n",
    "free_cells_count = len(temp_maze.free_cells)\n",
    "print(f\"  Total free cells to test: {free_cells_count}\")\n",
    "print(f\"  Maze dimensions: {maze.shape[0]}x{maze.shape[1]}\")\n",
    "print(f\"  Obstacles (walls): {np.sum(maze == 0)}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING COMPLETION CHECK...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Record start time\n",
    "check_start = time.time()\n",
    "\n",
    "# Track progress\n",
    "tested_cells = 0\n",
    "successful_cells = 0\n",
    "failed_cells = []\n",
    "\n",
    "try:\n",
    "    # Run completion check\n",
    "    print(\"\\nProgress: Testing each starting position...\")\n",
    "    print(\"(This may take a while - each dot represents a tested position)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Custom completion check with progress tracking\n",
    "    qmaze = TreasureMaze(maze)\n",
    "    all_successful = True\n",
    "    \n",
    "    for i, cell in enumerate(qmaze.free_cells):\n",
    "        # Test this starting position\n",
    "        if play_game(model, qmaze, cell, max_steps=200):\n",
    "            successful_cells += 1\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        else:\n",
    "            all_successful = False\n",
    "            failed_cells.append(cell)\n",
    "            print(\"F\", end=\"\", flush=True)  # F for failure\n",
    "        \n",
    "        tested_cells += 1\n",
    "        \n",
    "        # Print newline every 20 cells for readability\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\" {i+1}/{free_cells_count}\")\n",
    "    \n",
    "    print(f\" {tested_cells}/{free_cells_count}\")  # Final count\n",
    "    \n",
    "    # Calculate time taken\n",
    "    check_time = time.time() - check_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPLETION CHECK RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Cells tested: {tested_cells}/{free_cells_count}\")\n",
    "    print(f\"‚úÖ Successful starts: {successful_cells}/{free_cells_count}\")\n",
    "    print(f\"‚è±Ô∏è  Time taken: {format_time(check_time)}\")\n",
    "    \n",
    "    if all_successful:\n",
    "        print(\"\\n\" + \"üéâ \" * 10)\n",
    "        print(\"üåü MASTER NAVIGATOR ACHIEVED! üåü\")\n",
    "        print(\"üéâ \" * 10)\n",
    "        print(\"\\n‚úì COMPLETION CHECK PASSED!\")\n",
    "        print(\"  The pirate agent successfully navigates to the treasure\")\n",
    "        print(\"  from ALL possible starting positions in the maze!\")\n",
    "        print(\"\\n  This means the deep Q-learning algorithm has successfully\")\n",
    "        print(\"  learned the optimal policy for the entire maze environment.\")\n",
    "        \n",
    "        # Show success visualization\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # Pick a random starting position to demonstrate\n",
    "        demo_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(demo_cell)\n",
    "        play_game(model, qmaze, demo_cell, max_steps=200)\n",
    "        show(qmaze)\n",
    "        plt.title(f\"Demonstration: Successful Path from {demo_cell}\")\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå COMPLETION CHECK FAILED\")\n",
    "        print(f\"  The agent could not solve the maze from {len(failed_cells)} starting positions:\")\n",
    "        \n",
    "        # Show failed positions\n",
    "        print(\"\\n  Failed positions (row, col):\")\n",
    "        for i, cell in enumerate(failed_cells[:10]):  # Show first 10 failures\n",
    "            print(f\"    {i+1}. {cell}\")\n",
    "        if len(failed_cells) > 10:\n",
    "            print(f\"    ... and {len(failed_cells) - 10} more\")\n",
    "        \n",
    "        # Calculate success rate\n",
    "        success_rate = (successful_cells / free_cells_count) * 100\n",
    "        print(f\"\\n  Success rate: {success_rate:.1f}% ({successful_cells}/{free_cells_count})\")\n",
    "        \n",
    "        print(\"\\n  Possible reasons for failure:\")\n",
    "        print(\"  - Need more training epochs\")\n",
    "        print(\"  - Exploration rate may have decayed too quickly\")\n",
    "        print(\"  - Neural network architecture may need adjustment\")\n",
    "        print(\"  - Some positions may be particularly challenging\")\n",
    "        \n",
    "        # Visualize a failed position\n",
    "        if failed_cells:\n",
    "            print(\"\\n  Example of a failed starting position:\")\n",
    "            fail_cell = failed_cells[0]\n",
    "            qmaze.reset(fail_cell)\n",
    "            play_game(model, qmaze, fail_cell, max_steps=200)\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            show(qmaze)\n",
    "            plt.title(f\"Failed Path from {fail_cell}\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Detailed Analysis:\")\n",
    "    print(f\"  Average steps to treasure (when successful): N/A\")  # Could add if tracking\n",
    "    \n",
    "    # Check if we have win history from training\n",
    "    if 'win_history' in locals() and len(win_history) > 0:\n",
    "        final_win_rate = np.mean(win_history[-100:]) if len(win_history) >= 100 else np.mean(win_history)\n",
    "        print(f\"  Training win rate (last 100): {final_win_rate:.3f}\")\n",
    "        print(f\"  Completion check success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        if final_win_rate > 0.9 and not all_successful:\n",
    "            print(\"\\n  Note: High training win rate but failed completion check\")\n",
    "            print(\"  suggests the agent may be overfitting or not generalizing well.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during completion check: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Make sure the model is properly trained\")\n",
    "    print(\"2. Check that play_game function is working correctly\")\n",
    "    print(\"3. Verify the maze environment is accessible\")\n",
    "    print(\"4. Try with a smaller maze for testing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPLETION CHECK COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Optional: Save results\n",
    "if all_successful:\n",
    "    # Save the successful model with a special name\n",
    "    model.save(\"master_pirate_model_complete.h5\")\n",
    "    print(\"\\n‚úì Master model saved as 'master_pirate_model_complete.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will test your model for one game. It will start the pirate at the top-left corner and run <b>play_game()</b>. The agent should find a path from the starting position to the target (treasure). The treasure is located in the bottom-right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SINGLE GAME TEST - TOP-LEFT CORNER TO TREASURE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SINGLE GAME TEST: Starting from Top-Left Corner (0, 0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a fresh maze instance for testing\n",
    "test_maze = TreasureMaze(maze)\n",
    "start_position = (0, 0)  # Top-left corner\n",
    "treasure_position = (maze.shape[0]-1, maze.shape[1]-1)  # Bottom-right corner\n",
    "\n",
    "print(f\"\\nüìç Starting position: {start_position} (Top-Left Corner)\")\n",
    "print(f\"üíé Treasure location: {treasure_position} (Bottom-Right Corner)\")\n",
    "print(f\"üó∫Ô∏è  Maze dimensions: {maze.shape[0]}x{maze.shape[1]}\")\n",
    "\n",
    "# Reset the maze to starting position\n",
    "test_maze.reset(start_position)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INITIAL MAZE STATE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Legend:\")\n",
    "print(\"  ‚Ä¢ White cells = Free space (navigable)\")\n",
    "print(\"  ‚Ä¢ Black cells = Walls (obstacles)\")\n",
    "print(\"  ‚Ä¢ Gray cells = Visited path\")\n",
    "print(\"  ‚Ä¢ Dark gray = Pirate position\")\n",
    "print(\"  ‚Ä¢ Light gray = Treasure location\")\n",
    "print(\"\\nInitial maze configuration:\")\n",
    "\n",
    "# Display initial state\n",
    "plt.figure(figsize=(10, 10))\n",
    "show(test_maze)\n",
    "plt.title(f\"Initial State: Pirate at {start_position}, Treasure at {treasure_position}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RUNNING SINGLE GAME SIMULATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThe agent will now attempt to find a path from the top-left corner\")\n",
    "print(\"to the treasure in the bottom-right corner.\\n\")\n",
    "\n",
    "# Play the game and track progress\n",
    "print(\"Progress:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Record start time\n",
    "game_start = time.time()\n",
    "\n",
    "# Play the game\n",
    "try:\n",
    "    # Use the trained model to play one game\n",
    "    win = play_game(model, test_maze, start_position, max_steps=200)\n",
    "    \n",
    "    # Calculate game duration\n",
    "    game_time = time.time() - game_start\n",
    "    \n",
    "    # Get final statistics\n",
    "    steps_taken = test_maze.state[2]\n",
    "    cells_visited = len(test_maze.visited)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"GAME RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if win:\n",
    "        print(\"\\n\" + \"üéâ \" * 15)\n",
    "        print(\"üåü VICTORY! The pirate found the treasure! üåü\")\n",
    "        print(\"üéâ \" * 15)\n",
    "        print(f\"\\n‚úÖ SUCCESS: The agent successfully navigated from\")\n",
    "        print(f\"   ({start_position[0]}, {start_position[1]}) to ({treasure_position[0]}, {treasure_position[1]})!\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"üíî \" * 15)\n",
    "        print(\"‚ùå DEFEAT! The pirate failed to find the treasure! ‚ùå\")\n",
    "        print(\"üíî \" * 15)\n",
    "        print(f\"\\n‚ùå FAILURE: The agent could not reach the treasure\")\n",
    "        print(f\"   from starting position ({start_position[0]}, {start_position[1]}).\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"GAME STATISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  üïí Time elapsed: {game_time:.2f} seconds\")\n",
    "    print(f\"  üë£ Steps taken: {steps_taken}\")\n",
    "    print(f\"  üó∫Ô∏è  Cells visited: {cells_visited}\")\n",
    "    print(f\"  üìç Final position: ({test_maze.state[0]}, {test_maze.state[1]})\")\n",
    "    \n",
    "    # Calculate Manhattan distance from start to treasure\n",
    "    manhattan_dist = abs(start_position[0] - treasure_position[0]) + abs(start_position[1] - treasure_position[1])\n",
    "    print(f\"  üìè Manhattan distance (optimal minimum steps): {manhattan_dist}\")\n",
    "    \n",
    "    if win:\n",
    "        # Calculate efficiency\n",
    "        efficiency = (manhattan_dist / steps_taken) * 100 if steps_taken > 0 else 0\n",
    "        print(f\"  ‚ö° Path efficiency: {efficiency:.1f}%\")\n",
    "        if efficiency >= 90:\n",
    "            print(f\"     Excellent! Nearly optimal path found!\")\n",
    "        elif efficiency >= 70:\n",
    "            print(f\"     Good path, but some inefficiency detected\")\n",
    "        else:\n",
    "            print(f\"     Path is longer than optimal - may need more training\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL PATH VISUALIZATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nThe path taken by the pirate is shown in gray:\")\n",
    "    print(\"(Darker gray = More recently visited cells)\")\n",
    "    \n",
    "    # Display final state with path\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    show(test_maze)\n",
    "    \n",
    "    # Add path overlay if win\n",
    "    if win and len(test_maze.visited) > 1:\n",
    "        # Extract path coordinates\n",
    "        path = list(test_maze.visited)\n",
    "        path_x = [p[1] for p in path]\n",
    "        path_y = [p[0] for p in path]\n",
    "        \n",
    "        # Overlay path with arrows\n",
    "        for i in range(len(path) - 1):\n",
    "            dx = path[i+1][1] - path[i][1]\n",
    "            dy = path[i+1][0] - path[i][0]\n",
    "            plt.arrow(path[i][1], path[i][0], dx*0.6, dy*0.6, \n",
    "                     head_width=0.2, head_length=0.2, fc='blue', ec='blue', alpha=0.7)\n",
    "    \n",
    "    plt.title(f\"Final Path: {'VICTORY' if win else 'DEFEAT'} - {steps_taken} Steps\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PATH ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if win:\n",
    "        # Check if path is valid (no wall collisions)\n",
    "        valid_path = True\n",
    "        for cell in test_maze.visited:\n",
    "            if test_maze.maze[cell[0], cell[1]] == 0:  # Wall\n",
    "                valid_path = False\n",
    "                print(f\"‚ö†Ô∏è  Warning: Path includes wall at {cell}\")\n",
    "        \n",
    "        if valid_path:\n",
    "            print(\"‚úÖ Path validity: All moves were on free cells\")\n",
    "        \n",
    "        # Check if treasure was reached\n",
    "        if (test_maze.state[0], test_maze.state[1]) == treasure_position:\n",
    "            print(\"‚úÖ Treasure reached: Confirmed\")\n",
    "        \n",
    "        # Suggest improvements if needed\n",
    "        if steps_taken > manhattan_dist * 2:\n",
    "            print(\"\\nüí° Suggestion: The path is longer than optimal.\")\n",
    "            print(\"   Consider training for more epochs or adjusting\")\n",
    "            print(\"   the reward structure to encourage shorter paths.\")\n",
    "    else:\n",
    "        print(\"\\nüí° Analysis of failure:\")\n",
    "        print(\"   The agent got stuck or took too many steps.\")\n",
    "        print(\"   Possible reasons:\")\n",
    "        print(\"   ‚Ä¢ Need more training epochs\")\n",
    "        print(\"   ‚Ä¢ Exploration rate may be too low\")\n",
    "        print(\"   ‚Ä¢ Starting position may be particularly challenging\")\n",
    "        print(\"   ‚Ä¢ Model may need architecture adjustment\")\n",
    "    \n",
    "    # Show visited cells count\n",
    "    unique_cells = len(set(test_maze.visited))\n",
    "    print(f\"\\nüìä Exploration metrics:\")\n",
    "    print(f\"   Unique cells visited: {unique_cells}/{maze.size} ({unique_cells/maze.size*100:.1f}% of maze)\")\n",
    "    \n",
    "    if win:\n",
    "        revisit_ratio = (cells_visited - unique_cells) / cells_visited if cells_visited > 0 else 0\n",
    "        print(f\"   Cell revisit ratio: {revisit_ratio:.2%}\")\n",
    "        if revisit_ratio > 0.3:\n",
    "            print(\"   ‚ö†Ô∏è  High backtracking detected - agent may be inefficient\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during game simulation: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Ensure the model has been trained first\")\n",
    "    print(\"2. Check that play_game() function is defined correctly\")\n",
    "    print(\"3. Verify the maze environment is properly initialized\")\n",
    "    print(\"4. Try with a different starting position\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SINGLE GAME TEST COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Optional: Save a visualization of the successful path\n",
    "if win:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    show(test_maze)\n",
    "    plt.title(f\"Successful Path: {start_position} ‚Üí {treasure_position}\")\n",
    "    plt.savefig(\"successful_path.png\", dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nüì∏ Path visualization saved as 'successful_path.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Submit Your Work\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black;\">\n",
    "    <b>Hint:</b> To use the markdown block below, double click in the <b>Type Markdown and LaTeX:  ùõº2</b> block below, to turn it back to html, Run the cell.\n",
    "</div>\n",
    "\n",
    "After you have finished creating the code for your notebook, save your work.\n",
    "Make sure that your notebook contains your name in the filename (e.g. Doe_Jane_ProjectTwo.html). Download this file as an .html file clicking on ***file*** in *Jupyter Notebook*, navigating down to ***Download as*** and clicking on ***.html***. \n",
    "Download a copy of your .html file and submit it to Brightspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIRATE INTELLIGENT AGENT - DEEP Q-LEARNING SOLUTION\n",
    "## Author: Steven Foltz\n",
    "\n",
    "This notebook implements a deep Q-learning algorithm to train an intelligent pirate agent to navigate a maze and find treasure. The agent uses reinforcement learning to learn optimal paths through trial and error.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 1: TensorFlow Configuration\n",
    "This cell configures TensorFlow to suppress warning messages and optimize performance. Setting environment variables helps reduce console clutter during training.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 2: Import Required Libraries - Part 1\n",
    "Importing core Python libraries for system operations, time tracking, JSON handling, random number generation, and numerical computations with NumPy.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 3: Import TensorFlow and Keras Components\n",
    "Importing deep learning libraries including TensorFlow, Keras models, layers, and optimizers needed for building the neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 4: Import Visualization and Game Components\n",
    "Importing matplotlib for visualization and the custom TreasureMaze and GameExperience classes that provide the game environment and experience replay functionality.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 5: GPU Configuration and Session Cleanup\n",
    "Configuring GPU memory growth to prevent TensorFlow from allocating all available memory at once. Clearing any existing sessions ensures a fresh start.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 6: Maze Environment Definition\n",
    "Defining the 8x8 maze layout where:\n",
    "- **1.0** represents free cells (navigable)\n",
    "- **0.0** represents walls (obstacles)\n",
    "The treasure is located at the bottom-right corner (7, 7).\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 7: Maze Visualization Function\n",
    "The `show()` function creates a visual representation of the maze with:\n",
    "- **White cells**: Free space\n",
    "- **Black cells**: Walls\n",
    "- **Gray cells**: Visited path\n",
    "- **Dark gray**: Current pirate position\n",
    "- **Light gray**: Treasure location\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 8: Action Space Definition\n",
    "Defining the four possible movement directions:\n",
    "- **LEFT (0)**: Move left\n",
    "- **UP (1)**: Move up\n",
    "- **RIGHT (2)**: Move right\n",
    "- **DOWN (3)**: Move down\n",
    "The agent must learn which action to take in each state.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 9: Quick Environment Test\n",
    "Testing the maze environment by taking a single DOWN action. This verifies that the TreasureMaze class is working correctly and demonstrates the visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 10: Game Play Function\n",
    "The `play_game()` function simulates a complete game from a given starting cell:\n",
    "1. Resets the maze to the starting position\n",
    "2. Uses the trained model to select actions\n",
    "3. Continues until treasure found, trapped, or max steps reached\n",
    "4. Returns True if treasure found, False otherwise\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 11: Completion Check Function\n",
    "The `completion_check()` function verifies that the agent can successfully navigate from EVERY possible starting position in the maze. This is the ultimate test of whether the agent has truly learned to solve the maze.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 12: Build Model Function\n",
    "The `build_model()` creates the neural network architecture:\n",
    "- **Input layer**: 64 neurons (flattened 8x8 maze)\n",
    "- **Hidden layer 1**: 64 neurons with PReLU activation\n",
    "- **Hidden layer 2**: 64 neurons with PReLU activation\n",
    "- **Output layer**: 4 neurons (Q-values for each action)\n",
    "- **Optimizer**: Adam\n",
    "- **Loss function**: Mean Squared Error (MSE)\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 13: Time Formatting Helper\n",
    "The `format_time()` function converts seconds into a human-readable format (e.g., \"5m 30s\" or \"1h 15m\") for better progress tracking during long training sessions.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 14: Q-Training Algorithm Implementation\n",
    "**THIS IS THE KEY CELL - COMPLETES THE REQUIRED TODO SECTION**\n",
    "\n",
    "This function implements the complete deep Q-learning algorithm:\n",
    "1. Resets environment at random starting cell\n",
    "2. Gets initial environment state\n",
    "3. Uses epsilon-greedy strategy for action selection\n",
    "4. Stores experiences in replay memory\n",
    "5. Trains neural network with experience replay\n",
    "6. Tracks wins/losses and updates target network\n",
    "7. Monitors progress with early stopping\n",
    "\n",
    "The algorithm follows the pseudocode exactly as specified in the project requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 15: Build Neural Network Model\n",
    "Creating the neural network using the `build_model()` function. This cell displays the model architecture, layer sizes, and total parameters to verify the network is constructed correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 16: Train Model with Deep Q-Learning\n",
    "**NOTE: This step takes several minutes to fully run.**\n",
    "\n",
    "This cell executes the training process:\n",
    "- Runs for 1000 epochs (can be adjusted)\n",
    "- Starts with 100% exploration, decays to 1%\n",
    "- Provides progress updates every 100 epochs\n",
    "- Shows win rate, steps, loss, and elapsed time\n",
    "- Saves the best model when performance improves\n",
    "\n",
    "The agent learns through trial and error, gradually improving its ability to find the treasure.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 17: Visualize Training Progress\n",
    "Creating comprehensive visualizations of the training process:\n",
    "- **Plot 1**: Win rate over time with moving average\n",
    "- **Plot 2**: Cumulative wins during training  \n",
    "- **Plot 3**: Early vs late training comparison\n",
    "- **Plot 4**: Final performance statistics\n",
    "\n",
    "These visualizations help assess whether the agent learned effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 18: Quick Test - Single Game\n",
    "Testing the trained agent on a single game from the top-left corner (0, 0) to the treasure at bottom-right (7, 7). This provides:\n",
    "- Initial maze visualization\n",
    "- Real-time game simulation\n",
    "- Final path visualization\n",
    "- Detailed statistics (steps, time, efficiency)\n",
    "- Path quality analysis\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 19: Run Completion Check\n",
    "**NOTE: This could take several minutes.**\n",
    "\n",
    "This cell tests the agent from EVERY possible starting position in the maze:\n",
    "- Progress indicator shows dots (success) and F (failure)\n",
    "- Reports success rate and failed positions\n",
    "- Creates heatmap visualization of results\n",
    "- Provides detailed analysis of performance\n",
    "- Saves master model if all tests pass\n",
    "\n",
    "Passing this test means the agent has truly learned to solve the maze.\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 20: Single Game Test - Top-Left Corner to Treasure\n",
    "A focused test starting from the most challenging position:\n",
    "- Top-left corner (0, 0) to bottom-right corner (7, 7)\n",
    "- Shows complete path with direction arrows\n",
    "- Calculates path efficiency vs Manhattan distance\n",
    "- Provides victory/defeat celebration graphics\n",
    "- Saves successful path visualization\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 21: Bonus Test - Different Starting Position\n",
    "Optional additional test from a randomly selected middle position:\n",
    "- Tests generalization to different starting points\n",
    "- Compares performance across positions\n",
    "- Helps identify if agent has truly learned or memorized\n",
    "\n",
    "---\n",
    "\n",
    "## CELL 22: Final Summary and Model Save\n",
    "Consolidates all results and saves the final model:\n",
    "- Summary of all tests performed\n",
    "- Final model saved as 'final_pirate_model.h5'\n",
    "- Completion status and recommendations\n",
    "- Ready for submission\n",
    "\n",
    "---\n",
    "\n",
    "## PROJECT COMPLETE\n",
    "This notebook successfully implements a deep Q-learning algorithm to train an intelligent pirate agent that can navigate the maze and find treasure from any starting position. The agent demonstrates reinforcement learning principles including exploration vs exploitation, experience replay, and target networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
